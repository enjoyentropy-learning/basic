{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "joPVbC7LbQWV"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "wgqXUT-wxp5F"
      },
      "outputs": [],
      "source": [
        "class RawAttention(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.Wq = nn.Linear(1, 1, bias=False)\n",
        "        self.Wk = nn.Linear(1, 1, bias=False)\n",
        "        self.Wv = nn.Linear(1, 1, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch, 2)\n",
        "        x1 = x[:, 0:1]\n",
        "        x2 = x[:, 1:2]\n",
        "\n",
        "        tokens = torch.stack([x1, x2], dim=1)  # (batch, 2, 1)\n",
        "\n",
        "        Q = self.Wq(tokens)\n",
        "        K = self.Wk(tokens)\n",
        "        V = self.Wv(tokens)\n",
        "\n",
        "        scores = torch.matmul(Q, K.transpose(1, 2))  # (batch, 2, 2)\n",
        "        attended = torch.matmul(scores, V)           # (batch, 2, 1)\n",
        "\n",
        "        attended = attended.squeeze(-1)               # (batch, 2)\n",
        "\n",
        "        return attended\n",
        "\n",
        "class RawAttentionWithTopNeuron(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.raw_attention = RawAttention() # Instantiate the new RawAttention module\n",
        "        self.attn_out = nn.Linear(2, 1)   # attention readout. Bias is requried for Nand.\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch, 2)\n",
        "\n",
        "        attended = self.raw_attention(x) # Use the raw_attention module to get attended output\n",
        "\n",
        "        attn_output = self.attn_out(attended + x) # (batch, 1)\n",
        "\n",
        "        return attn_output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Hj0azwtwPtz9"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "# -----------------------------\n",
        "# Dataset: XOR\n",
        "# -----------------------------\n",
        "X = torch.tensor([\n",
        "    [0., 0.],\n",
        "    [0., 1.],\n",
        "    [1., 0.],\n",
        "    [1., 1.]\n",
        "])\n",
        "\n",
        "y = torch.tensor([[0.], [1.], [1.], [0.]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WILlVfXrelkK"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(0)\n",
        "\n",
        "# -----------------------------\n",
        "# Dataset: NAND\n",
        "# -----------------------------\n",
        "X = torch.tensor([\n",
        "    [0., 0.],\n",
        "    [0., 1.],\n",
        "    [1., 0.],\n",
        "    [1., 1.]\n",
        "])\n",
        "\n",
        "y = torch.tensor([[1.], [1.], [1.], [0.]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JftwpIFbaX0"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WbpJUTBKPJGd",
        "outputId": "ea97ac53-7634-4a9a-f22a-c23549910420"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0, loss = 0.8390\n",
            "Epoch 50, loss = 0.1130\n",
            "Epoch 100, loss = 0.0257\n",
            "Epoch 150, loss = 0.0036\n",
            "Epoch 200, loss = 0.0005\n",
            "Epoch 250, loss = 0.0001\n",
            "Epoch 300, loss = 0.0001\n",
            "Epoch 350, loss = 0.0000\n",
            "Epoch 400, loss = 0.0000\n",
            "Epoch 450, loss = 0.0000\n",
            "Epoch 500, loss = 0.0001\n",
            "Epoch 550, loss = 0.0000\n",
            "Epoch 600, loss = 0.0000\n",
            "Epoch 650, loss = 0.0000\n",
            "Epoch 700, loss = 0.0005\n",
            "Epoch 750, loss = 0.0001\n",
            "Epoch 800, loss = 0.0000\n",
            "Epoch 850, loss = 0.0000\n",
            "Epoch 900, loss = 0.0000\n",
            "Epoch 950, loss = 0.0004\n",
            "Epoch 1000, loss = 0.0000\n",
            "Epoch 1050, loss = 0.0000\n",
            "Epoch 1100, loss = 0.0000\n",
            "Epoch 1150, loss = 0.0000\n",
            "Epoch 1200, loss = 0.0000\n",
            "Epoch 1250, loss = 0.0001\n",
            "Epoch 1300, loss = 0.0000\n",
            "Epoch 1350, loss = 0.0000\n",
            "Epoch 1400, loss = 0.0000\n",
            "Epoch 1450, loss = 0.0000\n",
            "Epoch 1500, loss = 0.0000\n",
            "Epoch 1550, loss = 0.0000\n",
            "Epoch 1600, loss = 0.0000\n",
            "Epoch 1650, loss = 0.0000\n",
            "Epoch 1700, loss = 0.0000\n",
            "Epoch 1750, loss = 0.0032\n",
            "Epoch 1800, loss = 0.0000\n",
            "Epoch 1850, loss = 0.0000\n",
            "Epoch 1900, loss = 0.0000\n",
            "Epoch 1950, loss = 0.0000\n",
            "Epoch 2000, loss = 0.0023\n",
            "Epoch 2050, loss = 0.0000\n",
            "Epoch 2100, loss = 0.0000\n",
            "Epoch 2150, loss = 0.0000\n",
            "Epoch 2200, loss = 0.0000\n",
            "Epoch 2250, loss = 0.0000\n",
            "Epoch 2300, loss = 0.0000\n",
            "Epoch 2350, loss = 0.0000\n",
            "Epoch 2400, loss = 0.0000\n",
            "Epoch 2450, loss = 0.0000\n",
            "Epoch 2500, loss = 0.0005\n",
            "Epoch 2550, loss = 0.0000\n",
            "Epoch 2600, loss = 0.0000\n",
            "Epoch 2650, loss = 0.0000\n",
            "Epoch 2700, loss = 0.0000\n",
            "Epoch 2750, loss = 0.0002\n",
            "Epoch 2800, loss = 0.0000\n",
            "Epoch 2850, loss = 0.0000\n",
            "Epoch 2900, loss = 0.0000\n",
            "Epoch 2950, loss = 0.0000\n",
            "Epoch 3000, loss = 0.0001\n",
            "Epoch 3050, loss = 0.0000\n",
            "Epoch 3100, loss = 0.0000\n",
            "Epoch 3150, loss = 0.0000\n",
            "Epoch 3200, loss = 0.0001\n",
            "Epoch 3250, loss = 0.0000\n",
            "Epoch 3300, loss = 0.0000\n",
            "Epoch 3350, loss = 0.0000\n",
            "Epoch 3400, loss = 0.0000\n",
            "Epoch 3450, loss = 0.0002\n",
            "Epoch 3500, loss = 0.0000\n",
            "Epoch 3550, loss = 0.0000\n",
            "Epoch 3600, loss = 0.0000\n",
            "Epoch 3650, loss = 0.0000\n",
            "Epoch 3700, loss = 0.0006\n",
            "Epoch 3750, loss = 0.0000\n",
            "Epoch 3800, loss = 0.0000\n",
            "Epoch 3850, loss = 0.0000\n",
            "Epoch 3900, loss = 0.0000\n",
            "Epoch 3950, loss = 0.0001\n",
            "Epoch 4000, loss = 0.0000\n",
            "Epoch 4050, loss = 0.0000\n",
            "Epoch 4100, loss = 0.0000\n",
            "Epoch 4150, loss = 0.0007\n",
            "Epoch 4200, loss = 0.0000\n",
            "Epoch 4250, loss = 0.0000\n",
            "Epoch 4300, loss = 0.0000\n",
            "Epoch 4350, loss = 0.0000\n",
            "Epoch 4400, loss = 0.0001\n",
            "Epoch 4450, loss = 0.0000\n",
            "Epoch 4500, loss = 0.0000\n",
            "Epoch 4550, loss = 0.0000\n",
            "Epoch 4600, loss = 0.0000\n",
            "Epoch 4650, loss = 0.0002\n",
            "Epoch 4700, loss = 0.0000\n",
            "Epoch 4750, loss = 0.0000\n",
            "Epoch 4800, loss = 0.0000\n",
            "Epoch 4850, loss = 0.0000\n",
            "Epoch 4900, loss = 0.0000\n",
            "Epoch 4950, loss = 0.0001\n",
            "Epoch 5000, loss = 0.0000\n",
            "Epoch 5050, loss = 0.0000\n",
            "Epoch 5100, loss = 0.0000\n",
            "Epoch 5150, loss = 0.0007\n",
            "Epoch 5200, loss = 0.0000\n",
            "Epoch 5250, loss = 0.0000\n",
            "Epoch 5300, loss = 0.0000\n",
            "Epoch 5350, loss = 0.0001\n",
            "Epoch 5400, loss = 0.0000\n",
            "Epoch 5450, loss = 0.0000\n",
            "Epoch 5500, loss = 0.0000\n",
            "Epoch 5550, loss = 0.0008\n",
            "Epoch 5600, loss = 0.0000\n",
            "Epoch 5650, loss = 0.0000\n",
            "Epoch 5700, loss = 0.0000\n",
            "Epoch 5750, loss = 0.0000\n",
            "Epoch 5800, loss = 0.0000\n",
            "Epoch 5850, loss = 0.0000\n",
            "Epoch 5900, loss = 0.0000\n",
            "Epoch 5950, loss = 0.0000\n",
            "Epoch 6000, loss = 0.0000\n",
            "Epoch 6050, loss = 0.0000\n",
            "Epoch 6100, loss = 0.0000\n",
            "Epoch 6150, loss = 0.0000\n",
            "Epoch 6200, loss = 0.0001\n",
            "Epoch 6250, loss = 0.0000\n",
            "Epoch 6300, loss = 0.0000\n",
            "Epoch 6350, loss = 0.0000\n",
            "Epoch 6400, loss = 0.0000\n",
            "Epoch 6450, loss = 0.0000\n",
            "Epoch 6500, loss = 0.0000\n",
            "Epoch 6550, loss = 0.0000\n",
            "Epoch 6600, loss = 0.0000\n",
            "Epoch 6650, loss = 0.0029\n",
            "Epoch 6700, loss = 0.0000\n",
            "Epoch 6750, loss = 0.0000\n",
            "Epoch 6800, loss = 0.0000\n",
            "Epoch 6850, loss = 0.0000\n",
            "Epoch 6900, loss = 0.0028\n",
            "Epoch 6950, loss = 0.0000\n",
            "Epoch 7000, loss = 0.0000\n",
            "Epoch 7050, loss = 0.0000\n",
            "Epoch 7100, loss = 0.0000\n",
            "Epoch 7150, loss = 0.0000\n",
            "Epoch 7200, loss = 0.0000\n",
            "Epoch 7250, loss = 0.0000\n",
            "Epoch 7300, loss = 0.0000\n",
            "Epoch 7350, loss = 0.0000\n",
            "Epoch 7400, loss = 0.0000\n",
            "Epoch 7450, loss = 0.0000\n",
            "Epoch 7500, loss = 0.0000\n",
            "Epoch 7550, loss = 0.0000\n",
            "Epoch 7600, loss = 0.0000\n",
            "Epoch 7650, loss = 0.0000\n",
            "Epoch 7700, loss = 0.0000\n",
            "Epoch 7750, loss = 0.0004\n",
            "Epoch 7800, loss = 0.0000\n",
            "Epoch 7850, loss = 0.0000\n",
            "Epoch 7900, loss = 0.0000\n",
            "Epoch 7950, loss = 0.0000\n"
          ]
        }
      ],
      "source": [
        "model = RawAttentionWithTopNeuron()\n",
        "\n",
        "\n",
        "#optimizer = optim.Adam(model.parameters(), lr=0.05)\n",
        "optimizer = optim.AdamW(model.parameters(), lr=5e-2) #5e-2 is same as .05\n",
        "#optimizer = torch.optim.SGD(model.parameters(), lr=0.05)\n",
        "\n",
        "\n",
        "# Tried learning rate 1e-3, but for this problem it is not working.\n",
        "# Using scheduler below helps, but since it is fluctuating with .00x, may not be worth it.\n",
        "#scheduler = torch.optim.lr_scheduler.ExponentialLR(\n",
        "#    optimizer,\n",
        "#    gamma=0.9995\n",
        "#)\n",
        "#optimizer = optim.Adam(model.parameters(), lr=0.05)\n",
        "\n",
        "loss_fn = nn.MSELoss()\n",
        "loss_history = []\n",
        "for epoch in range(8000):\n",
        "    optimizer.zero_grad()\n",
        "    preds = model(X)\n",
        "    loss = loss_fn(preds, y)\n",
        "    loss_history.append(loss.item()) # Store loss\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    #scheduler.step()\n",
        "\n",
        "    if epoch % 50 == 0:\n",
        "        print(f\"Epoch {epoch}, loss = {loss.item():.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mt70gmraJagh",
        "outputId": "ed668456-e64d-4ac2-fde0-a77988937bef"
      },
      "outputs": [],
      "source": [
        "model = RawAttentionWithTopNeuron()\n",
        "optimizer = torch.optim.LBFGS(\n",
        "    model.parameters(),\n",
        "    lr=0.05,\n",
        "    max_iter=20,\n",
        "    history_size=100\n",
        ")\n",
        "\n",
        "loss_fn = nn.MSELoss()\n",
        "loss_history = []\n",
        "\n",
        "for epoch in range(8000):\n",
        "\n",
        "    def closure():\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(X)\n",
        "        loss = loss_fn(preds, y)\n",
        "        loss.backward()\n",
        "        return loss\n",
        "\n",
        "    loss = optimizer.step(closure)\n",
        "    loss_history.append(loss.item())\n",
        "\n",
        "    if epoch % 50 == 0:\n",
        "        print(f\"Epoch {epoch}, loss = {loss.item():.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lP3yTXRkMBVZ",
        "outputId": "07221dd2-5e88-420d-b79c-022f8882fa7f"
      },
      "outputs": [],
      "source": [
        "# Apply newton method on the loss function itself as we know Loss should be zero\n",
        "# multiply and divide as we can't divide by gradient which is vector\n",
        "model = RawAttentionWithTopNeuron()\n",
        "\n",
        "loss_fn = nn.MSELoss()\n",
        "loss_history = []\n",
        "\n",
        "for epoch in range(800):\n",
        "    # Zero gradients\n",
        "    model.zero_grad()\n",
        "\n",
        "    # Forward\n",
        "    preds = model(X)\n",
        "    loss = loss_fn(preds, y)\n",
        "    loss_history.append(loss.item())\n",
        "\n",
        "    # Backward\n",
        "    loss.backward()\n",
        "\n",
        "    # ---- YOUR UPDATE RULE ----\n",
        "    with torch.no_grad():\n",
        "        # Compute ||grad||^2 over ALL parameters\n",
        "        grad_norm_sq = 0.0\n",
        "        for p in model.parameters():\n",
        "            if p.grad is not None:\n",
        "                grad_norm_sq += torch.sum(p.grad ** 2)\n",
        "\n",
        "        # Safety check\n",
        "        if grad_norm_sq < 1e-12:\n",
        "            print(\"Gradient vanished — stopping.\")\n",
        "            break\n",
        "\n",
        "        step_scale = loss / grad_norm_sq\n",
        "\n",
        "        for p in model.parameters():\n",
        "            if p.grad is not None:\n",
        "                p -= step_scale * p.grad\n",
        "    # --------------------------\n",
        "\n",
        "    if epoch % 1 == 0:\n",
        "        print(f\"Epoch {epoch}, loss = {loss.item():.6f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMg0g95c927T",
        "outputId": "d7331721-6823-41e7-977c-4ac47dba5760"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "losses = np.array(loss_history)\n",
        "THRESHOLD = 0.0001\n",
        "\n",
        "# Find first epoch where loss goes below threshold\n",
        "below_idx = np.where(losses <= THRESHOLD)[0]\n",
        "\n",
        "if len(below_idx) == 0:\n",
        "    print(\"Loss never reached threshold.\")\n",
        "else:\n",
        "    start = below_idx[0]\n",
        "    increases = np.where(losses[start+1:] > losses[start:-1])[0]\n",
        "\n",
        "    if len(increases) == 0:\n",
        "        print(\"No loss increases after convergence.\")\n",
        "    else:\n",
        "        print(f\"Loss increased {len(increases)} times after reaching {THRESHOLD}.\")\n",
        "        for i in increases[:10]:  # show first few\n",
        "            e = start + i + 1\n",
        "            print(\n",
        "                f\"Epoch {e}: \"\n",
        "                f\"{losses[e-1]:.6f} → {losses[e]:.6f}\"\n",
        "            )\n",
        "    print(f\"Loss at last {losses[-1]:.12f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "23d6c280",
        "outputId": "c4d39c20-d59d-48cf-dd63-0363bd1b8edd"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "#plt.plot(loss_history[:1500], label='Training Loss')\n",
        "plt.plot(loss_history[400:], label='Training Loss')\n",
        "#plt.plot(loss_history[:500], label='Training Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss History')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_S8mYTR0kqI",
        "outputId": "0f519f3a-cc22-4180-9217-45bd244e9743"
      },
      "outputs": [],
      "source": [
        "print('Learned Parameters:')\n",
        "for name, param in model.named_parameters():\n",
        "    print(f'Parameter: {name}, Value: {param.data}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48d1dbb8"
      },
      "source": [
        "### Numerical Substitution for Gate\n",
        "\n",
        "Using the learned parameters for the NAND gate:\n",
        "\n",
        "*   $w_q = 0.6367$\n",
        "*   $w_k = 1.1251$\n",
        "*   $w_v = -1.3959$\n",
        "*   $w_{attn\\_out,1} = 0.4634$\n",
        "*   $w_{attn\\_out,2} = 0.5366$\n",
        "*   $b_{attn\\_out} = 1.0000$\n",
        "\n",
        "First, calculate $C = w_q w_k w_v = (0.6367) \\cdot (1.1251) \\cdot (-1.3959) \\approx -0.9995 \\approx -1.0$.\n",
        "\n",
        "Substituting these values into the simplified formula for binary inputs $(x_1^2 = x_1, x_2^2 = x_2)$:\n",
        "\n",
        "$output = (w_{attn\\_out,1} \\cdot x_1 + w_{attn\\_out,2} \\cdot x_2) \\cdot (C \\cdot (x_1 + x_2) + 1) + b_{attn\\_out}$\n",
        "\n",
        "becomes:\n",
        "\n",
        "$output = (0.4634 \\cdot x_1 + 0.5366 \\cdot x_2) \\cdot (-1.0 \\cdot (x_1 + x_2) + 1) + 1.0$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fffe9b4",
        "outputId": "4f1b54c2-99d3-4bd0-9494-4d244ef41180"
      },
      "outputs": [],
      "source": [
        "wq = model.raw_attention.Wq.weight.item()\n",
        "wk = model.raw_attention.Wk.weight.item()\n",
        "wv = model.raw_attention.Wv.weight.item()\n",
        "w_attn_out1 = model.attn_out.weight[0,0].item()\n",
        "w_attn_out2 = model.attn_out.weight[0,1].item()\n",
        "b_attn_out = model.attn_out.bias[0].item()\n",
        "#b_attn_out = 0\n",
        "C = wq * wk * wv\n",
        "\n",
        "# Calculate coefficients for the fully expanded form for explanation\n",
        "coeff_x1_term = w_attn_out1 * (C + 1)\n",
        "coeff_x2_term = w_attn_out2 * (C + 1)\n",
        "coeff_x1x2_term = C * (w_attn_out1 + w_attn_out2)\n",
        "print(coeff_x1_term, coeff_x2_term, coeff_x1x2_term, b_attn_out)\n",
        "print(f\"{coeff_x1_term:.4f}, {coeff_x2_term:.4f}, {coeff_x1x2_term:.4f}, {b_attn_out:.4f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "401d9ea3"
      },
      "source": [
        "### Numerical Substitution\n",
        "\n",
        "Using the learned parameters (for XOR:\n",
        "\n",
        "*   $w_q = -1.0343$\n",
        "*   $w_k = -0.7918$\n",
        "*   $w_v = -0.6119$\n",
        "*   $w_{attn\\_out,1} = 1.9998$\n",
        "*   $w_{attn\\_out,2} = 1.9997$\n",
        "*   $b_{attn\\_out} = -0.0005$\n",
        "\n",
        "First, calculate $C = w_q w_k w_v = (-1.0343) \\cdot (-0.7918) \\cdot (-0.6119) \\approx -0.5011$.\n",
        "\n",
        "Substituting these values into the simplified formula:\n",
        "\n",
        "$output = (1.9998 \\cdot x_1 + 1.9997 \\cdot x_2) \\cdot (-0.5011 \\cdot (x_1^2 + x_2^2) + 1) - 0.0005$\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
