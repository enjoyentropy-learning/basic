{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run debugLogicGates.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Training with AdamW Optimizer (float32) ---\")\n",
    "torch.manual_seed(0)\n",
    "torch.set_default_dtype(dtype_float32)\n",
    "\n",
    "X, y = get_gate_data('xor', dtype=dtype_float32)\n",
    "model_adamw, loss_fn_adamw = initialize_model_and_loss(dtype=dtype_float32)\n",
    "optimizer_adamw = optim.AdamW(model_adamw.parameters(), lr=0.05)\n",
    "\n",
    "adamw_loss_history = train_model(model_adamw, optimizer_adamw, loss_fn_adamw, X, y,\n",
    "                                 epochs=8000, log_interval=500, optimizer_type='adamw')\n",
    "evaluate_model(model_adamw, loss_fn_adamw, X, y, gate_name='NAND (AdamW, float32)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Training with LBFGS Optimizer (float64) ---\")\n",
    "torch.manual_seed(0)\n",
    "torch.set_default_dtype(dtype_float64)\n",
    "\n",
    "X, y = get_gate_data('xor', dtype=dtype_float64)\n",
    "model_lbfgs, loss_fn_lbfgs = initialize_model_and_loss(dtype=dtype_float64)\n",
    "optimizer_lbfgs = torch.optim.LBFGS(model_lbfgs.parameters(), lr=0.05, max_iter=20, history_size=100)\n",
    "\n",
    "lbfgs_loss_history = train_model(model_lbfgs, optimizer_lbfgs, loss_fn_lbfgs, X, y,\n",
    "                                 epochs=8000, log_interval=500, optimizer_type='lbfgs')\n",
    "evaluate_model(model_lbfgs, loss_fn_lbfgs, X, y, gate_name='NAND (LBFGS, float64)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Training with Custom Optimizer (float32) ---\")\n",
    "torch.manual_seed(0)\n",
    "torch.set_default_dtype(dtype_float32)\n",
    "\n",
    "X, y = get_gate_data('xor', dtype=dtype_float32)\n",
    "model_custom, loss_fn_custom = initialize_model_and_loss(dtype=dtype_float32)\n",
    "\n",
    "custom_loss_history = train_model(model_custom, None, loss_fn_custom, X, y,\n",
    "                                  epochs=800, log_interval=100, optimizer_type='custom',\n",
    "                                  custom_update_fn=custom_gradient_update)\n",
    "evaluate_model(model_custom, loss_fn_custom, X, y, gate_name='NAND (Custom, float32)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(adamw_loss_history, label='AdamW Loss (float32)', alpha=0.7)\n",
    "plt.plot(lbfgs_loss_history, label='LBFGS Loss (float64)', alpha=0.7)\n",
    "plt.plot(custom_loss_history, label='Custom Update Loss (float32)', alpha=0.7)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
